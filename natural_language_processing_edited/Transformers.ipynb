{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTnlh8ck+QUPxbMfu9inpd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nathan-Mekuria-Solomon/ML-practice/blob/main/natural_language_processing_edited/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bO3UPpvs-mFE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create positional embedding layer\n",
        "class PositionalEncoding(keras.layers.Layer):\n",
        "  def __init__(self, max_steps, max_dims, dtype= tf.float32, **kwargs):\n",
        "    super().__init__(dtype= dtype, **kwargs)\n",
        "    if max_dims % 2 == 1: max_dims+= 1 # max_dims must be even (why?)\n",
        "    p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims))\n",
        "    pos_embed = np.empty((1, max_steps, max_dims)) # on the book (1, max_steps, max_dims // 2)\n",
        "    pos_embed[::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
        "    pos_embed[1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
        "    self.positional_embedding = tf.constant(pos_embed.astype(self.dtype))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    shape = tf.shape(inputs)\n",
        "    return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n"
      ],
      "metadata": {
        "id": "00LeSI67-v0E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first layer of the transformer\n",
        "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
        "encoder_inputs = keras.layers.Input(shape= [None], dtype= np.int32)\n",
        "decoder_inputs = keras.layers.Input(shape= [None], dtype= np.int32)\n",
        "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
        "encoder_embeddings = embeddings(encoder_inputs)\n",
        "decoder_embeddings = embeddings(decoder_inputs)\n",
        "positional_encoding = PositionalEncoding(max_steps, max_dims= embed_size)\n",
        "encoder_in = positional_encoding(encoder_embeddings)\n",
        "decoder_in = positional_encoding(decoder_embeddings)"
      ],
      "metadata": {
        "id": "8yea0RC2FjkD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing a transformer\n",
        "Z = encoder_in\n",
        "for N in range(6):\n",
        "  Z = keras.layers.Attention(use_scale= True)([Z, Z])\n",
        "\n",
        "encoder_outputs = Z\n",
        "Z = decoder_in\n",
        "for N in range(6):\n",
        "  Z = keras.layers.Attention(use_scale= True)([Z, Z])\n",
        "  Z = keras.layers.Attention(use_scale= True)([Z, encoder_outputs])\n",
        "\n",
        "outputs = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation= \"softmax\"))(Z)"
      ],
      "metadata": {
        "id": "Ti3x4y6BH5CS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EF--hdrysFNR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}